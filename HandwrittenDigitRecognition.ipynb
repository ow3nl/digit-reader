{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Seed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7fef916ec890>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random_seed = 1\n",
    "torch.backends.cudnn.enabled = False\n",
    "torch.manual_seed(random_seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_epochs = 3\n",
    "batch_size_train = 64\n",
    "batch_size_test = 1000\n",
    "learning_rate = 0.01\n",
    "momentum = 0.5\n",
    "log_interval = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_loader = torch.utils.data.DataLoader(\n",
    "  torchvision.datasets.MNIST('MNIST', train=True, download=True,\n",
    "                             transform=torchvision.transforms.Compose([\n",
    "                               torchvision.transforms.ToTensor(),\n",
    "                               torchvision.transforms.Normalize(\n",
    "                                 (0.1307,), (0.3081,))\n",
    "                             ])),\n",
    "  batch_size=batch_size_train, shuffle=True)\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "  torchvision.datasets.MNIST('MNIST', train=False, download=True,\n",
    "                             transform=torchvision.transforms.Compose([\n",
    "                               torchvision.transforms.ToTensor(),\n",
    "                               torchvision.transforms.Normalize(\n",
    "                                 (0.1307,), (0.3081,))\n",
    "                             ])),\n",
    "  batch_size=batch_size_test, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 10, kernel_size=5)\n",
    "        self.bn1 = nn.BatchNorm2d(10)\n",
    "        self.conv2 = nn.Conv2d(10, 20, kernel_size=5)\n",
    "        self.conv2_drop = nn.Dropout2d()\n",
    "        self.bn2 = nn.BatchNorm2d(20)\n",
    "        self.fc1 = nn.Linear(320, 50)\n",
    "        self.bn3 = nn.BatchNorm1d(50)\n",
    "        self.fc2 = nn.Linear(50, 10)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = F.max_pool2d(self.conv1(x), 2)\n",
    "        x = F.relu(self.bn1(x))\n",
    "        x = F.max_pool2d(self.conv2_drop(self.conv2(x)), 2)\n",
    "        x = F.relu(self.bn2(x))\n",
    "        x = x.view(-1, 320)\n",
    "        x = F.relu(self.bn3(self.fc1(x)))\n",
    "        x = F.dropout(x, training=self.training)\n",
    "        x = F.relu(self.fc2(x))\n",
    "        return F.log_softmax(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "network = Net()\n",
    "\n",
    "optimizer = optim.SGD(network.parameters(), lr=learning_rate, momentum=momentum)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_losses = []\n",
    "train_counter = []\n",
    "test_losses = []\n",
    "test_counter = [i*len(train_loader.dataset) for i in range(n_epochs + 1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(epoch):\n",
    "    network.train()\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        optimizer.zero_grad()\n",
    "        output = network(data)\n",
    "        loss = F.nll_loss(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if batch_idx % log_interval == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                epoch, batch_idx * len(data), len(train_loader.dataset), \n",
    "                100. * batch_idx / len(train_loader), loss.item()))\n",
    "            train_losses.append(loss.item())\n",
    "            train_counter.append((batch_idx*64) + ((epoch-1)*len(train_loader.dataset)))\n",
    "            torch.save(network.state_dict(), 'SavedNN')\n",
    "            torch.save(optimizer.state_dict(), 'SavedNN')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test():\n",
    "    network.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        for data, target in test_loader:\n",
    "            output = network(data)\n",
    "            test_loss += F.nll_loss(output, target, size_average=False).item()\n",
    "            pred = output.data.max(1, keepdim=True)[1]\n",
    "            correct += pred.eq(target.data.view_as(pred)).sum()\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "    print('\\nTest set: Avg. loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
    "        test_loss, correct, len(test_loader.dataset), 100. * correct / len(test_loader.dataset)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-5-07bcadd7aa37>:23: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  return F.log_softmax(x)\n",
      "/Users/owen/opt/anaconda3/lib/python3.8/site-packages/torch/nn/_reduction.py:44: UserWarning: size_average and reduce args will be deprecated, please use reduction='sum' instead.\n",
      "  warnings.warn(warning.format(ret))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Avg. loss: 2.3148, Accuracy: 1137/10000 (11%)\n",
      "\n",
      "Train Epoch: 1 [0/60000 (0%)]\tLoss: 2.398904\n",
      "Train Epoch: 1 [640/60000 (1%)]\tLoss: 2.273256\n",
      "Train Epoch: 1 [1280/60000 (2%)]\tLoss: 2.246839\n",
      "Train Epoch: 1 [1920/60000 (3%)]\tLoss: 2.114421\n",
      "Train Epoch: 1 [2560/60000 (4%)]\tLoss: 2.107560\n",
      "Train Epoch: 1 [3200/60000 (5%)]\tLoss: 1.894486\n",
      "Train Epoch: 1 [3840/60000 (6%)]\tLoss: 1.868422\n",
      "Train Epoch: 1 [4480/60000 (7%)]\tLoss: 1.860377\n",
      "Train Epoch: 1 [5120/60000 (9%)]\tLoss: 1.780147\n",
      "Train Epoch: 1 [5760/60000 (10%)]\tLoss: 1.609790\n",
      "Train Epoch: 1 [6400/60000 (11%)]\tLoss: 1.895836\n",
      "Train Epoch: 1 [7040/60000 (12%)]\tLoss: 1.614323\n",
      "Train Epoch: 1 [7680/60000 (13%)]\tLoss: 1.520074\n",
      "Train Epoch: 1 [8320/60000 (14%)]\tLoss: 1.567617\n",
      "Train Epoch: 1 [8960/60000 (15%)]\tLoss: 1.398483\n",
      "Train Epoch: 1 [9600/60000 (16%)]\tLoss: 1.393346\n",
      "Train Epoch: 1 [10240/60000 (17%)]\tLoss: 1.332331\n",
      "Train Epoch: 1 [10880/60000 (18%)]\tLoss: 1.252467\n",
      "Train Epoch: 1 [11520/60000 (19%)]\tLoss: 1.088183\n",
      "Train Epoch: 1 [12160/60000 (20%)]\tLoss: 1.175401\n",
      "Train Epoch: 1 [12800/60000 (21%)]\tLoss: 1.226829\n",
      "Train Epoch: 1 [13440/60000 (22%)]\tLoss: 1.194809\n",
      "Train Epoch: 1 [14080/60000 (23%)]\tLoss: 1.245218\n",
      "Train Epoch: 1 [14720/60000 (25%)]\tLoss: 1.053290\n",
      "Train Epoch: 1 [15360/60000 (26%)]\tLoss: 1.218111\n",
      "Train Epoch: 1 [16000/60000 (27%)]\tLoss: 1.279507\n",
      "Train Epoch: 1 [16640/60000 (28%)]\tLoss: 1.076988\n",
      "Train Epoch: 1 [17280/60000 (29%)]\tLoss: 1.043253\n",
      "Train Epoch: 1 [17920/60000 (30%)]\tLoss: 0.902655\n",
      "Train Epoch: 1 [18560/60000 (31%)]\tLoss: 1.059439\n",
      "Train Epoch: 1 [19200/60000 (32%)]\tLoss: 1.011210\n",
      "Train Epoch: 1 [19840/60000 (33%)]\tLoss: 0.911979\n",
      "Train Epoch: 1 [20480/60000 (34%)]\tLoss: 1.011327\n",
      "Train Epoch: 1 [21120/60000 (35%)]\tLoss: 0.889605\n",
      "Train Epoch: 1 [21760/60000 (36%)]\tLoss: 1.004781\n",
      "Train Epoch: 1 [22400/60000 (37%)]\tLoss: 0.903561\n",
      "Train Epoch: 1 [23040/60000 (38%)]\tLoss: 0.785741\n",
      "Train Epoch: 1 [23680/60000 (39%)]\tLoss: 0.909516\n",
      "Train Epoch: 1 [24320/60000 (41%)]\tLoss: 0.611137\n",
      "Train Epoch: 1 [24960/60000 (42%)]\tLoss: 0.948385\n",
      "Train Epoch: 1 [25600/60000 (43%)]\tLoss: 0.722328\n",
      "Train Epoch: 1 [26240/60000 (44%)]\tLoss: 0.664241\n",
      "Train Epoch: 1 [26880/60000 (45%)]\tLoss: 0.686940\n",
      "Train Epoch: 1 [27520/60000 (46%)]\tLoss: 0.521470\n",
      "Train Epoch: 1 [28160/60000 (47%)]\tLoss: 0.725195\n",
      "Train Epoch: 1 [28800/60000 (48%)]\tLoss: 0.610541\n",
      "Train Epoch: 1 [29440/60000 (49%)]\tLoss: 0.695425\n",
      "Train Epoch: 1 [30080/60000 (50%)]\tLoss: 0.841361\n",
      "Train Epoch: 1 [30720/60000 (51%)]\tLoss: 0.516477\n",
      "Train Epoch: 1 [31360/60000 (52%)]\tLoss: 0.496054\n",
      "Train Epoch: 1 [32000/60000 (53%)]\tLoss: 0.542866\n",
      "Train Epoch: 1 [32640/60000 (54%)]\tLoss: 0.646563\n",
      "Train Epoch: 1 [33280/60000 (55%)]\tLoss: 0.755410\n",
      "Train Epoch: 1 [33920/60000 (57%)]\tLoss: 0.483934\n",
      "Train Epoch: 1 [34560/60000 (58%)]\tLoss: 0.582308\n",
      "Train Epoch: 1 [35200/60000 (59%)]\tLoss: 0.633100\n",
      "Train Epoch: 1 [35840/60000 (60%)]\tLoss: 0.606923\n",
      "Train Epoch: 1 [36480/60000 (61%)]\tLoss: 0.670392\n",
      "Train Epoch: 1 [37120/60000 (62%)]\tLoss: 0.509258\n",
      "Train Epoch: 1 [37760/60000 (63%)]\tLoss: 0.590416\n",
      "Train Epoch: 1 [38400/60000 (64%)]\tLoss: 0.508080\n",
      "Train Epoch: 1 [39040/60000 (65%)]\tLoss: 0.736274\n",
      "Train Epoch: 1 [39680/60000 (66%)]\tLoss: 0.558539\n",
      "Train Epoch: 1 [40320/60000 (67%)]\tLoss: 0.410462\n",
      "Train Epoch: 1 [40960/60000 (68%)]\tLoss: 0.514252\n",
      "Train Epoch: 1 [41600/60000 (69%)]\tLoss: 0.587355\n",
      "Train Epoch: 1 [42240/60000 (70%)]\tLoss: 0.610705\n",
      "Train Epoch: 1 [42880/60000 (71%)]\tLoss: 0.512299\n",
      "Train Epoch: 1 [43520/60000 (72%)]\tLoss: 0.536572\n",
      "Train Epoch: 1 [44160/60000 (74%)]\tLoss: 0.495584\n",
      "Train Epoch: 1 [44800/60000 (75%)]\tLoss: 0.415789\n",
      "Train Epoch: 1 [45440/60000 (76%)]\tLoss: 0.569410\n",
      "Train Epoch: 1 [46080/60000 (77%)]\tLoss: 0.434563\n",
      "Train Epoch: 1 [46720/60000 (78%)]\tLoss: 0.520602\n",
      "Train Epoch: 1 [47360/60000 (79%)]\tLoss: 0.405969\n",
      "Train Epoch: 1 [48000/60000 (80%)]\tLoss: 0.383492\n",
      "Train Epoch: 1 [48640/60000 (81%)]\tLoss: 0.526474\n",
      "Train Epoch: 1 [49280/60000 (82%)]\tLoss: 0.448854\n",
      "Train Epoch: 1 [49920/60000 (83%)]\tLoss: 0.326190\n",
      "Train Epoch: 1 [50560/60000 (84%)]\tLoss: 0.407516\n",
      "Train Epoch: 1 [51200/60000 (85%)]\tLoss: 0.546795\n",
      "Train Epoch: 1 [51840/60000 (86%)]\tLoss: 0.490457\n",
      "Train Epoch: 1 [52480/60000 (87%)]\tLoss: 0.522575\n",
      "Train Epoch: 1 [53120/60000 (88%)]\tLoss: 0.388442\n",
      "Train Epoch: 1 [53760/60000 (90%)]\tLoss: 0.509914\n",
      "Train Epoch: 1 [54400/60000 (91%)]\tLoss: 0.353710\n",
      "Train Epoch: 1 [55040/60000 (92%)]\tLoss: 0.546587\n",
      "Train Epoch: 1 [55680/60000 (93%)]\tLoss: 0.449264\n",
      "Train Epoch: 1 [56320/60000 (94%)]\tLoss: 0.339441\n",
      "Train Epoch: 1 [56960/60000 (95%)]\tLoss: 0.432204\n",
      "Train Epoch: 1 [57600/60000 (96%)]\tLoss: 0.290362\n",
      "Train Epoch: 1 [58240/60000 (97%)]\tLoss: 0.491080\n",
      "Train Epoch: 1 [58880/60000 (98%)]\tLoss: 0.366118\n",
      "Train Epoch: 1 [59520/60000 (99%)]\tLoss: 0.444411\n",
      "\n",
      "Test set: Avg. loss: 0.1305, Accuracy: 9696/10000 (97%)\n",
      "\n",
      "Train Epoch: 2 [0/60000 (0%)]\tLoss: 0.346008\n",
      "Train Epoch: 2 [640/60000 (1%)]\tLoss: 0.650310\n",
      "Train Epoch: 2 [1280/60000 (2%)]\tLoss: 0.384482\n",
      "Train Epoch: 2 [1920/60000 (3%)]\tLoss: 0.232571\n",
      "Train Epoch: 2 [2560/60000 (4%)]\tLoss: 0.348614\n",
      "Train Epoch: 2 [3200/60000 (5%)]\tLoss: 0.391750\n",
      "Train Epoch: 2 [3840/60000 (6%)]\tLoss: 0.466588\n",
      "Train Epoch: 2 [4480/60000 (7%)]\tLoss: 0.387047\n",
      "Train Epoch: 2 [5120/60000 (9%)]\tLoss: 0.386355\n",
      "Train Epoch: 2 [5760/60000 (10%)]\tLoss: 0.447133\n",
      "Train Epoch: 2 [6400/60000 (11%)]\tLoss: 0.433340\n",
      "Train Epoch: 2 [7040/60000 (12%)]\tLoss: 0.307690\n",
      "Train Epoch: 2 [7680/60000 (13%)]\tLoss: 0.446008\n",
      "Train Epoch: 2 [8320/60000 (14%)]\tLoss: 0.348329\n",
      "Train Epoch: 2 [8960/60000 (15%)]\tLoss: 0.328564\n",
      "Train Epoch: 2 [9600/60000 (16%)]\tLoss: 0.391749\n",
      "Train Epoch: 2 [10240/60000 (17%)]\tLoss: 0.323957\n",
      "Train Epoch: 2 [10880/60000 (18%)]\tLoss: 0.385559\n",
      "Train Epoch: 2 [11520/60000 (19%)]\tLoss: 0.372573\n",
      "Train Epoch: 2 [12160/60000 (20%)]\tLoss: 0.518741\n",
      "Train Epoch: 2 [12800/60000 (21%)]\tLoss: 0.420196\n",
      "Train Epoch: 2 [13440/60000 (22%)]\tLoss: 0.333280\n",
      "Train Epoch: 2 [14080/60000 (23%)]\tLoss: 0.282620\n",
      "Train Epoch: 2 [14720/60000 (25%)]\tLoss: 0.394778\n",
      "Train Epoch: 2 [15360/60000 (26%)]\tLoss: 0.384918\n",
      "Train Epoch: 2 [16000/60000 (27%)]\tLoss: 0.299194\n",
      "Train Epoch: 2 [16640/60000 (28%)]\tLoss: 0.309328\n",
      "Train Epoch: 2 [17280/60000 (29%)]\tLoss: 0.404188\n",
      "Train Epoch: 2 [17920/60000 (30%)]\tLoss: 0.329140\n",
      "Train Epoch: 2 [18560/60000 (31%)]\tLoss: 0.448157\n",
      "Train Epoch: 2 [19200/60000 (32%)]\tLoss: 0.361788\n",
      "Train Epoch: 2 [19840/60000 (33%)]\tLoss: 0.258927\n",
      "Train Epoch: 2 [20480/60000 (34%)]\tLoss: 0.214180\n",
      "Train Epoch: 2 [21120/60000 (35%)]\tLoss: 0.277302\n",
      "Train Epoch: 2 [21760/60000 (36%)]\tLoss: 0.147155\n",
      "Train Epoch: 2 [22400/60000 (37%)]\tLoss: 0.285402\n",
      "Train Epoch: 2 [23040/60000 (38%)]\tLoss: 0.355537\n",
      "Train Epoch: 2 [23680/60000 (39%)]\tLoss: 0.323064\n",
      "Train Epoch: 2 [24320/60000 (41%)]\tLoss: 0.389962\n",
      "Train Epoch: 2 [24960/60000 (42%)]\tLoss: 0.275738\n",
      "Train Epoch: 2 [25600/60000 (43%)]\tLoss: 0.348867\n",
      "Train Epoch: 2 [26240/60000 (44%)]\tLoss: 0.331566\n",
      "Train Epoch: 2 [26880/60000 (45%)]\tLoss: 0.275498\n",
      "Train Epoch: 2 [27520/60000 (46%)]\tLoss: 0.321988\n",
      "Train Epoch: 2 [28160/60000 (47%)]\tLoss: 0.291759\n",
      "Train Epoch: 2 [28800/60000 (48%)]\tLoss: 0.417319\n",
      "Train Epoch: 2 [29440/60000 (49%)]\tLoss: 0.226677\n",
      "Train Epoch: 2 [30080/60000 (50%)]\tLoss: 0.359312\n",
      "Train Epoch: 2 [30720/60000 (51%)]\tLoss: 0.256971\n",
      "Train Epoch: 2 [31360/60000 (52%)]\tLoss: 0.336008\n",
      "Train Epoch: 2 [32000/60000 (53%)]\tLoss: 0.200126\n",
      "Train Epoch: 2 [32640/60000 (54%)]\tLoss: 0.334933\n",
      "Train Epoch: 2 [33280/60000 (55%)]\tLoss: 0.396781\n",
      "Train Epoch: 2 [33920/60000 (57%)]\tLoss: 0.397262\n",
      "Train Epoch: 2 [34560/60000 (58%)]\tLoss: 0.218304\n",
      "Train Epoch: 2 [35200/60000 (59%)]\tLoss: 0.433441\n",
      "Train Epoch: 2 [35840/60000 (60%)]\tLoss: 0.448543\n",
      "Train Epoch: 2 [36480/60000 (61%)]\tLoss: 0.307534\n",
      "Train Epoch: 2 [37120/60000 (62%)]\tLoss: 0.246845\n",
      "Train Epoch: 2 [37760/60000 (63%)]\tLoss: 0.351056\n",
      "Train Epoch: 2 [38400/60000 (64%)]\tLoss: 0.294904\n",
      "Train Epoch: 2 [39040/60000 (65%)]\tLoss: 0.461922\n",
      "Train Epoch: 2 [39680/60000 (66%)]\tLoss: 0.250038\n",
      "Train Epoch: 2 [40320/60000 (67%)]\tLoss: 0.302034\n",
      "Train Epoch: 2 [40960/60000 (68%)]\tLoss: 0.326811\n",
      "Train Epoch: 2 [41600/60000 (69%)]\tLoss: 0.294603\n",
      "Train Epoch: 2 [42240/60000 (70%)]\tLoss: 0.334729\n",
      "Train Epoch: 2 [42880/60000 (71%)]\tLoss: 0.429483\n",
      "Train Epoch: 2 [43520/60000 (72%)]\tLoss: 0.335395\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 2 [44160/60000 (74%)]\tLoss: 0.228100\n",
      "Train Epoch: 2 [44800/60000 (75%)]\tLoss: 0.399407\n",
      "Train Epoch: 2 [45440/60000 (76%)]\tLoss: 0.263324\n",
      "Train Epoch: 2 [46080/60000 (77%)]\tLoss: 0.469536\n",
      "Train Epoch: 2 [46720/60000 (78%)]\tLoss: 0.232521\n",
      "Train Epoch: 2 [47360/60000 (79%)]\tLoss: 0.215254\n",
      "Train Epoch: 2 [48000/60000 (80%)]\tLoss: 0.308776\n",
      "Train Epoch: 2 [48640/60000 (81%)]\tLoss: 0.175527\n",
      "Train Epoch: 2 [49280/60000 (82%)]\tLoss: 0.334403\n",
      "Train Epoch: 2 [49920/60000 (83%)]\tLoss: 0.215701\n",
      "Train Epoch: 2 [50560/60000 (84%)]\tLoss: 0.448807\n",
      "Train Epoch: 2 [51200/60000 (85%)]\tLoss: 0.371213\n",
      "Train Epoch: 2 [51840/60000 (86%)]\tLoss: 0.304635\n",
      "Train Epoch: 2 [52480/60000 (87%)]\tLoss: 0.205673\n",
      "Train Epoch: 2 [53120/60000 (88%)]\tLoss: 0.546345\n",
      "Train Epoch: 2 [53760/60000 (90%)]\tLoss: 0.180386\n",
      "Train Epoch: 2 [54400/60000 (91%)]\tLoss: 0.385592\n",
      "Train Epoch: 2 [55040/60000 (92%)]\tLoss: 0.216952\n",
      "Train Epoch: 2 [55680/60000 (93%)]\tLoss: 0.469579\n",
      "Train Epoch: 2 [56320/60000 (94%)]\tLoss: 0.258225\n",
      "Train Epoch: 2 [56960/60000 (95%)]\tLoss: 0.241518\n",
      "Train Epoch: 2 [57600/60000 (96%)]\tLoss: 0.190737\n",
      "Train Epoch: 2 [58240/60000 (97%)]\tLoss: 0.365947\n",
      "Train Epoch: 2 [58880/60000 (98%)]\tLoss: 0.381321\n",
      "Train Epoch: 2 [59520/60000 (99%)]\tLoss: 0.257816\n",
      "\n",
      "Test set: Avg. loss: 0.0794, Accuracy: 9772/10000 (98%)\n",
      "\n",
      "Train Epoch: 3 [0/60000 (0%)]\tLoss: 0.461908\n",
      "Train Epoch: 3 [640/60000 (1%)]\tLoss: 0.212571\n",
      "Train Epoch: 3 [1280/60000 (2%)]\tLoss: 0.202946\n",
      "Train Epoch: 3 [1920/60000 (3%)]\tLoss: 0.284547\n",
      "Train Epoch: 3 [2560/60000 (4%)]\tLoss: 0.144328\n",
      "Train Epoch: 3 [3200/60000 (5%)]\tLoss: 0.285328\n",
      "Train Epoch: 3 [3840/60000 (6%)]\tLoss: 0.198128\n",
      "Train Epoch: 3 [4480/60000 (7%)]\tLoss: 0.200383\n",
      "Train Epoch: 3 [5120/60000 (9%)]\tLoss: 0.239516\n",
      "Train Epoch: 3 [5760/60000 (10%)]\tLoss: 0.292611\n",
      "Train Epoch: 3 [6400/60000 (11%)]\tLoss: 0.383825\n",
      "Train Epoch: 3 [7040/60000 (12%)]\tLoss: 0.267900\n",
      "Train Epoch: 3 [7680/60000 (13%)]\tLoss: 0.226168\n",
      "Train Epoch: 3 [8320/60000 (14%)]\tLoss: 0.233972\n",
      "Train Epoch: 3 [8960/60000 (15%)]\tLoss: 0.237009\n",
      "Train Epoch: 3 [9600/60000 (16%)]\tLoss: 0.251411\n",
      "Train Epoch: 3 [10240/60000 (17%)]\tLoss: 0.456485\n",
      "Train Epoch: 3 [10880/60000 (18%)]\tLoss: 0.339257\n",
      "Train Epoch: 3 [11520/60000 (19%)]\tLoss: 0.512262\n",
      "Train Epoch: 3 [12160/60000 (20%)]\tLoss: 0.347783\n",
      "Train Epoch: 3 [12800/60000 (21%)]\tLoss: 0.286171\n",
      "Train Epoch: 3 [13440/60000 (22%)]\tLoss: 0.350329\n",
      "Train Epoch: 3 [14080/60000 (23%)]\tLoss: 0.363426\n",
      "Train Epoch: 3 [14720/60000 (25%)]\tLoss: 0.278368\n",
      "Train Epoch: 3 [15360/60000 (26%)]\tLoss: 0.221815\n",
      "Train Epoch: 3 [16000/60000 (27%)]\tLoss: 0.186591\n",
      "Train Epoch: 3 [16640/60000 (28%)]\tLoss: 0.339023\n",
      "Train Epoch: 3 [17280/60000 (29%)]\tLoss: 0.214450\n",
      "Train Epoch: 3 [17920/60000 (30%)]\tLoss: 0.163165\n",
      "Train Epoch: 3 [18560/60000 (31%)]\tLoss: 0.185130\n",
      "Train Epoch: 3 [19200/60000 (32%)]\tLoss: 0.324618\n",
      "Train Epoch: 3 [19840/60000 (33%)]\tLoss: 0.165981\n",
      "Train Epoch: 3 [20480/60000 (34%)]\tLoss: 0.282775\n",
      "Train Epoch: 3 [21120/60000 (35%)]\tLoss: 0.242345\n",
      "Train Epoch: 3 [21760/60000 (36%)]\tLoss: 0.283793\n",
      "Train Epoch: 3 [22400/60000 (37%)]\tLoss: 0.316749\n",
      "Train Epoch: 3 [23040/60000 (38%)]\tLoss: 0.188745\n",
      "Train Epoch: 3 [23680/60000 (39%)]\tLoss: 0.200593\n",
      "Train Epoch: 3 [24320/60000 (41%)]\tLoss: 0.324662\n",
      "Train Epoch: 3 [24960/60000 (42%)]\tLoss: 0.174090\n",
      "Train Epoch: 3 [25600/60000 (43%)]\tLoss: 0.309005\n",
      "Train Epoch: 3 [26240/60000 (44%)]\tLoss: 0.301154\n",
      "Train Epoch: 3 [26880/60000 (45%)]\tLoss: 0.285822\n",
      "Train Epoch: 3 [27520/60000 (46%)]\tLoss: 0.176609\n",
      "Train Epoch: 3 [28160/60000 (47%)]\tLoss: 0.278227\n",
      "Train Epoch: 3 [28800/60000 (48%)]\tLoss: 0.224376\n",
      "Train Epoch: 3 [29440/60000 (49%)]\tLoss: 0.192451\n",
      "Train Epoch: 3 [30080/60000 (50%)]\tLoss: 0.203369\n",
      "Train Epoch: 3 [30720/60000 (51%)]\tLoss: 0.170770\n",
      "Train Epoch: 3 [31360/60000 (52%)]\tLoss: 0.104692\n",
      "Train Epoch: 3 [32000/60000 (53%)]\tLoss: 0.391981\n",
      "Train Epoch: 3 [32640/60000 (54%)]\tLoss: 0.446291\n",
      "Train Epoch: 3 [33280/60000 (55%)]\tLoss: 0.322893\n",
      "Train Epoch: 3 [33920/60000 (57%)]\tLoss: 0.254116\n",
      "Train Epoch: 3 [34560/60000 (58%)]\tLoss: 0.144167\n",
      "Train Epoch: 3 [35200/60000 (59%)]\tLoss: 0.231998\n",
      "Train Epoch: 3 [35840/60000 (60%)]\tLoss: 0.221802\n",
      "Train Epoch: 3 [36480/60000 (61%)]\tLoss: 0.192971\n",
      "Train Epoch: 3 [37120/60000 (62%)]\tLoss: 0.341679\n",
      "Train Epoch: 3 [37760/60000 (63%)]\tLoss: 0.357867\n",
      "Train Epoch: 3 [38400/60000 (64%)]\tLoss: 0.149771\n",
      "Train Epoch: 3 [39040/60000 (65%)]\tLoss: 0.193448\n",
      "Train Epoch: 3 [39680/60000 (66%)]\tLoss: 0.198064\n",
      "Train Epoch: 3 [40320/60000 (67%)]\tLoss: 0.162373\n",
      "Train Epoch: 3 [40960/60000 (68%)]\tLoss: 0.287671\n",
      "Train Epoch: 3 [41600/60000 (69%)]\tLoss: 0.234817\n",
      "Train Epoch: 3 [42240/60000 (70%)]\tLoss: 0.151865\n",
      "Train Epoch: 3 [42880/60000 (71%)]\tLoss: 0.343805\n",
      "Train Epoch: 3 [43520/60000 (72%)]\tLoss: 0.241577\n",
      "Train Epoch: 3 [44160/60000 (74%)]\tLoss: 0.255983\n",
      "Train Epoch: 3 [44800/60000 (75%)]\tLoss: 0.229226\n",
      "Train Epoch: 3 [45440/60000 (76%)]\tLoss: 0.232333\n",
      "Train Epoch: 3 [46080/60000 (77%)]\tLoss: 0.246299\n",
      "Train Epoch: 3 [46720/60000 (78%)]\tLoss: 0.170611\n",
      "Train Epoch: 3 [47360/60000 (79%)]\tLoss: 0.289954\n",
      "Train Epoch: 3 [48000/60000 (80%)]\tLoss: 0.254192\n",
      "Train Epoch: 3 [48640/60000 (81%)]\tLoss: 0.195329\n",
      "Train Epoch: 3 [49280/60000 (82%)]\tLoss: 0.245778\n",
      "Train Epoch: 3 [49920/60000 (83%)]\tLoss: 0.150676\n",
      "Train Epoch: 3 [50560/60000 (84%)]\tLoss: 0.240517\n",
      "Train Epoch: 3 [51200/60000 (85%)]\tLoss: 0.179381\n",
      "Train Epoch: 3 [51840/60000 (86%)]\tLoss: 0.214708\n",
      "Train Epoch: 3 [52480/60000 (87%)]\tLoss: 0.182631\n",
      "Train Epoch: 3 [53120/60000 (88%)]\tLoss: 0.234608\n",
      "Train Epoch: 3 [53760/60000 (90%)]\tLoss: 0.429261\n",
      "Train Epoch: 3 [54400/60000 (91%)]\tLoss: 0.318577\n",
      "Train Epoch: 3 [55040/60000 (92%)]\tLoss: 0.295168\n",
      "Train Epoch: 3 [55680/60000 (93%)]\tLoss: 0.216259\n",
      "Train Epoch: 3 [56320/60000 (94%)]\tLoss: 0.342720\n",
      "Train Epoch: 3 [56960/60000 (95%)]\tLoss: 0.214742\n",
      "Train Epoch: 3 [57600/60000 (96%)]\tLoss: 0.221995\n",
      "Train Epoch: 3 [58240/60000 (97%)]\tLoss: 0.441751\n",
      "Train Epoch: 3 [58880/60000 (98%)]\tLoss: 0.329933\n",
      "Train Epoch: 3 [59520/60000 (99%)]\tLoss: 0.220503\n",
      "\n",
      "Test set: Avg. loss: 0.0696, Accuracy: 9794/10000 (98%)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test()\n",
    "for epoch in range(1, n_epochs+1):\n",
    "    train(epoch)\n",
    "    test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-5-07bcadd7aa37>:23: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  return F.log_softmax(x)\n"
     ]
    }
   ],
   "source": [
    "from PIL import Image\n",
    "from PIL import ImageEnhance\n",
    "import PIL.ImageOps\n",
    "import numpy as np\n",
    "\n",
    "original_data = False\n",
    "\n",
    "if original_data:\n",
    "    im = Image.open('/Users/owen/Desktop/Digit.jpg')\n",
    "    im = im.resize((28, 28))\n",
    "    im = im.convert('L')\n",
    "    im = PIL.ImageOps.invert(im)\n",
    "\n",
    "    enhancer = ImageEnhance.Contrast(im)\n",
    "    im = enhancer.enhance(5)\n",
    "    \n",
    "    im_data = np.array(im).transpose()\n",
    "else: \n",
    "    examples = enumerate(test_loader)\n",
    "    batch_idx, (example_data, example_targets) = next(examples)\n",
    "    im_data = example_data[0][0].numpy().transpose()\n",
    "\n",
    "im_tensor = torch.Tensor(im_data)\n",
    "im_tensor = im_tensor.unsqueeze(0).unsqueeze(0)\n",
    "\n",
    "output = network(im_tensor)\n",
    "pred = int(output.data.max(1, keepdim=True)[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 1.0, 'Prediction: 8')"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig = plt.figure()\n",
    "plt.imshow(im_data.transpose(), cmap='gray', interpolation='none')\n",
    "plt.title(f'Prediction: {pred}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
